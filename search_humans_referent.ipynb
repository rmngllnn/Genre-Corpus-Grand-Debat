{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c6c906-ee5a-40e3-81df-974cccb04b68",
   "metadata": {},
   "source": [
    "# Qu'est-ce qu'il y a dans ce notebook ?\n",
    "\n",
    "Fonctions permettant de calculer et créer un dictionnaire de similarité entre chaque occurence unique de mots  \n",
    "Fonctions permettant d'extraire, pour chaque occurence unique, le mot le plus similaire et de récupérer son nombre d'occurence  \n",
    "Fonctions permettant de recréer un dictionnaire d'occurence pour faciliter l'extraction en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facc0792-8256-4ce8-ac8c-1b619731d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False\n",
    "serialized_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e232c8-8167-482b-b3f4-51ad906964ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "#from ipynb.fs.defs.fonctions_preprocess import open_file\n",
    "#from ipynb.fs.defs.fonctions_preprocess import serialisation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5c8c8-fb57-495e-b9bd-09689a28c9cc",
   "metadata": {},
   "source": [
    "## Extraction et mise en forme du corpus\n",
    "\n",
    "On extrait le corpus à partir du fichier souhaité (selon fichier en partie sérialisé ou non)  \n",
    "Mise en format du fichier : nécessité de passer d'une liste de mot à une string pour que le module SpaCy fonctionne  \n",
    "Sérialisation de l'objet string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985c3d2c-9015-454e-b4e7-fc64f33042c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file):\n",
    "    with open(file) as json_data:\n",
    "        data = json.load(json_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f15ea64-e6aa-4215-9696-19140ddee2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_string from json file\n",
      "corpus extracted\n"
     ]
    }
   ],
   "source": [
    "if serialized_data == False:\n",
    "    print(\"extracting whole data from noun corpus\")\n",
    "    data = open_file(\"nouns.json\")\n",
    "    print(len(data))\n",
    "else:\n",
    "    print(\"Extracting data_string from json file\")\n",
    "    corpus = open_file(\"corpus_string.json\")\n",
    "    print(\"corpus extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba47d2c-b7e5-46bb-8c1a-18f9064d1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    test = corpus[0:200]\n",
    "    print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d26b9-de6d-4be4-9930-ad52681ab0b3",
   "metadata": {},
   "source": [
    "#### Extraction des noms du tuple et création de la string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725743ff-4645-4290-8c8e-38f3e14788e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_into_string(tuple_list):\n",
    "\n",
    "    extract = []\n",
    "    extract_s = \"\"\n",
    "\n",
    "    #Permet d'enlever les doublons\n",
    "    for tup in tuple_list:\n",
    "        if tup[0] not in extract:\n",
    "            extract.append(tup[0])\n",
    "            \n",
    "    print(\"Taille des occurences uniques : \", len(extract))\n",
    "        \n",
    "    for word in extract:\n",
    "        extract_s += word\n",
    "        extract_s += \" \"\n",
    "        \n",
    "    return extract_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7088c7-9932-4c17-9f86-2a8909be21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if serialized_data == False:\n",
    "    corpus = list_into_string(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ec1a40-de9c-4613-a1a9-a98652d9b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if serialized_data == False:\n",
    "    serialisation_data (corpus, \"corpus_string.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef7acc-0e12-47a1-9cd0-44c594df3c38",
   "metadata": {},
   "source": [
    "## Calcul de similarités via le module SpaCy\n",
    "\n",
    "Chargement du module avec les vecteurs pré-entrainés `fr_core_news_md`  \n",
    "Chargement du corpus dans deux documents pour s'assurer que tous les tokens seront lus ensemble  \n",
    "Fonctions permettant le calcul de similarité et construction d'un dictionnaire de similarité  \n",
    "Sérialisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93f8fd-f385-405f-8f7e-8a71a4c7126b",
   "metadata": {},
   "source": [
    "#### Chargement du module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd625e2-acf4-4be6-90b2-d2c53dae1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85e1c7c-5692-4a91-b7eb-eb2a71df8d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dealing with whole corpus\n"
     ]
    }
   ],
   "source": [
    "nlp.max_length = 1500000\n",
    "\n",
    "if debug_mode:\n",
    "    print(\"debug mode : corpus test\")\n",
    "    doc1 = nlp(test)\n",
    "    doc2 = nlp(test)\n",
    "else:\n",
    "    print(\"dealing with whole corpus\")\n",
    "    doc1 = nlp(corpus)\n",
    "    doc2 = nlp(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2982765-32e9-4ef4-b259-60b62e7562c9",
   "metadata": {},
   "source": [
    "#### Fonctions permettant le calcul de similarité\n",
    "\n",
    "`calculate_similarity` calcule la similarité entre deux mots  \n",
    "`construction_list_similarity` construit une liste de similarité constituée de tuples avec deux mots et leur similarité  \n",
    "`extract_unique_referent` permet, à partir de la liste de similarité, de construire un dictionnaire avec, pour chaque occurence, ses similarités les plus fortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be1b941-eac2-4ee5-afc5-34c249eebd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity (token1, token2):\n",
    "    \"\"\"Calculate Similarity between two tokens using SpaCy pre-trained model\n",
    "    -> Two tokens\n",
    "    <- similarity score between these two tokens\"\"\"\n",
    "\n",
    "    similarity = token1.similarity(token2)\n",
    "    \n",
    "    if similarity >= 0.5 and similarity != 1 : return (token1, token2, similarity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ccc1a5-37cd-4833-87df-8952578f4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construction_liste_similarity(doc1, doc2):\n",
    "    \"\"\"Builds list of similarities. One element of the list is built so : (token1, token2, similarity)\n",
    "    -> doc1 : list of words\n",
    "    -> doc2 : other list of words\n",
    "    <- list of tuples made of two words and their similarities\n",
    "    \"\"\"\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for i in range (len(doc1)):\n",
    "        if doc1[i].has_vector != False: tok = doc1[i]\n",
    "        for k in range(len(doc2)):\n",
    "            if doc2[k].has_vector != False : \n",
    "                similarity = calculate_similarity(tok, doc2[k])\n",
    "                if similarity != None : similarities.append(similarity)\n",
    "                    \n",
    "    return similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a85be26-20c9-4b28-a8f8-fc28667ed870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_referents(doc1, doc2):\n",
    "    \"\"\"extract unique_referent using word_similarities\n",
    "    -> string of words\n",
    "    <- list of words and dictionnary of similarities\n",
    "    doc_sim = {word1: [(word, similarity), ..., (word, similarity)], word2 : [(word, similarity), ... (word, similarity)], ... wordn : [(word, similarity), (word, similarity)]}\n",
    "    \n",
    "    We use word similarities using spacy module. We take similarities >= 0.5 to delete low similarities and \"clean\" corpus) \n",
    "    \"\"\"\n",
    "    \n",
    "    humans_referent = []\n",
    "    doc_sim = {}\n",
    "\n",
    "    similarities = construction_liste_similarity(doc1, doc2)\n",
    "\n",
    "    print(\"Similarities are done !\")\n",
    "    \n",
    "    i=0\n",
    "    for sim in similarities:\n",
    "        i+=1\n",
    "        if i%1000 == 0: print(\"Similarité n°\", i, \"/\", len(similarities) )\n",
    "        if sim[1] not in humans_referent: humans_referent.append((sim[1])) \n",
    "        if sim[0] not in doc_sim : \n",
    "            doc_sim[sim[0]] = []\n",
    "        doc_sim[sim[0]].append((sim[1], sim[2]))\n",
    "        \n",
    "\n",
    "    print(\"Taille de humans_referent : \", len(humans_referent), \" et taille de doc_sim : \", len(doc_sim))\n",
    "    return (humans_referent, doc_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9761d254-19c9-4648-af29-200b5307d8cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-104b6254856b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_unique_referents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-86715680b57f>\u001b[0m in \u001b[0;36mextract_unique_referents\u001b[0;34m(doc1, doc2)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdoc_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruction_liste_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Similarities are done !\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8875fd42368b>\u001b[0m in \u001b[0;36mconstruction_liste_similarity\u001b[0;34m(doc1, doc2)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_vector\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-773ce2c7bfd6>\u001b[0m in \u001b[0;36mcalculate_similarity\u001b[0;34m(token1, token2)\u001b[0m\n\u001b[1;32m      4\u001b[0m     <- similarity score between these two tokens\"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "extraction = extract_unique_referents(doc1, doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de44e5-9b03-48f1-9747-4ba8fad2f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    for dic in extraction[1]:\n",
    "        print(\"dic :\", dic, \" -> \", extraction[1][dic])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afcb9e-2415-4138-95e4-9a24b7bc7e93",
   "metadata": {},
   "source": [
    "#### Sérialisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56004ba-1fb3-4255-92e9-df8b0c59d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialisation_data (data, title):\n",
    "  \"\"\"\n",
    "  Serialize data in a json file\n",
    "  -> Title mus be a string : title.json\n",
    "  <- Save a file in desktop\n",
    "  \"\"\"\n",
    "\n",
    "  with open(title, \"w+\") as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3aa8a2-98f3-4021-9b7d-66feb25e4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialisation_data(extraction[0], \"list_human_ref.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1c28b-9954-4aab-8cb1-11ff81a7198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialisation_data(extraction[1], \"dic_sim_human_ref.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29313654-f2f5-4163-bb56-d5bb8eaf67a1",
   "metadata": {},
   "source": [
    "## Extraction des référents humains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019a51d-a50b-4fff-97ba-1b831023c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humans_occ (dict_occ, dict_sim):\n",
    "    \"\"\"Creates dict of occurences\n",
    "    -> dict_occ : dictionnary of occcurences of each occurence\n",
    "    -> dict_human_ref : dictionnary of similarities of each occurence\n",
    "    <- dictionnary of occurences of most occurence\n",
    "    \"\"\"\n",
    "    \n",
    "    humans_ref = []\n",
    "    humans_occ = {}\n",
    "    \n",
    "    \n",
    "    for dic in dict_sim:\n",
    "        sim_ord = sorted(dict_sim[dic])\n",
    "        ref_max = sim_ord[0]\n",
    "        if ref_max not in humans_ref : humans_ref.append(ref_max[1])\n",
    "            \n",
    "        if ref_max not in humans_occ: \n",
    "            humans_occ[ref_max] = dict_occ[ref_max]\n",
    "        \n",
    "    return dict_occ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
