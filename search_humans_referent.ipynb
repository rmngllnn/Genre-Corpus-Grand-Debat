{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "facc0792-8256-4ce8-ac8c-1b619731d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34e232c8-8167-482b-b3f4-51ad906964ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from ipynb.fs.defs.fonctions_preprocess import open_file\n",
    "from ipynb.fs.defs.fonctions_preprocess import serialisation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f15ea64-e6aa-4215-9696-19140ddee2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13803483\n"
     ]
    }
   ],
   "source": [
    "data = open_file(\"nouns.json\")\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba47d2c-b7e5-46bb-8c1a-18f9064d1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    test = data[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725743ff-4645-4290-8c8e-38f3e14788e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_into_string(tuple_list):\n",
    "\n",
    "    extract = []\n",
    "    extract_s = \"\"\n",
    "\n",
    "    #Permet d'enlever les doublons\n",
    "    for tup in tuple_list:\n",
    "        if tup[0] not in extract:\n",
    "            extract.append(tup[0])\n",
    "            \n",
    "    print(\"Taille des occurences uniques : \", len(extract))\n",
    "        \n",
    "    for word in extract:\n",
    "        extract_s += word\n",
    "        extract_s += \" \"\n",
    "        \n",
    "    return extract_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7088c7-9932-4c17-9f86-2a8909be21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des occurences uniques :  132725\n"
     ]
    }
   ],
   "source": [
    "corpus = list_into_string(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ec1a40-de9c-4613-a1a9-a98652d9b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialisation_data (corpus, \"corpus_string.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd625e2-acf4-4be6-90b2-d2c53dae1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c85e1c7c-5692-4a91-b7eb-eb2a71df8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1500000\n",
    "\n",
    "doc1 = nlp(corpus)\n",
    "doc2 = nlp(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6be1b941-eac2-4ee5-afc5-34c249eebd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity (token1, token2):\n",
    "    \"\"\"Calculate Similarity between two tokens using SpaCy pre-trained model\n",
    "    -> Two tokens\n",
    "    <- similarity score between these two tokens\"\"\"\n",
    "\n",
    "    similarity = token1.similarity(token2)\n",
    "    \n",
    "    if similarity >= 0.5 and similarity != 1 : return (token1, token2, similarity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ccc1a5-37cd-4833-87df-8952578f4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construction_liste_similarity(doc1, doc2):\n",
    "    \"\"\"Builds list of similarities. One element of the list is built so : (token1, token2, similarity)\n",
    "    -> doc1 : list of words\n",
    "    -> doc2 : other list of words\n",
    "    <- list of tuples made of two words and their similarities\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for i in range (len(doc1)):\n",
    "        if doc1[i].has_vector != False: tok = doc1[i]\n",
    "        for k in range(len(doc2)):\n",
    "            if doc2[k].has_vector != False : \n",
    "                similarity = calculate_similarity(tok, doc2[k])\n",
    "                if similarity != None : similarities.append(similarity)\n",
    "                    \n",
    "    return similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a85be26-20c9-4b28-a8f8-fc28667ed870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_referents(doc1, doc2):\n",
    "    \"\"\"extract unique_referent using word_similarities\n",
    "    -> string of words\n",
    "    <- list of words and dictionnary of similarities\n",
    "    doc_sim = {word1: [(word, similarity), ..., (word, similarity)], word2 : [(word, similarity), ... (word, similarity)], ... wordn : [(word, similarity), (word, similarity)]}\n",
    "    \n",
    "    We use word similarities using spacy module. We take similarities >= 0.5 to delete low similarities and \"clean\" corpus) \n",
    "    \"\"\"\n",
    "    \n",
    "    humans_referent = []\n",
    "    doc_sim = {}\n",
    "\n",
    "    similarities = construction_liste_similarity(doc1, doc2)\n",
    "\n",
    "    print(\"Similarities are done !\")\n",
    "    \n",
    "    for sim in similarities:\n",
    "        if sim[1] not in humans_referent: humans_referent.append((sim[1])) \n",
    "        if sim[0] not in doc_sim : \n",
    "            doc_sim[sim[0]] = []\n",
    "        doc_sim[sim[0]].append((sim[1], sim[2]))\n",
    "\n",
    "    print(\"Taillle de humans_referent : \", len(humans_referent), \" et taille de doc_sim : \", len(doc_sim))\n",
    "    return (humans_referent, doc_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9761d254-19c9-4648-af29-200b5307d8cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-104b6254856b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_unique_referents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-e8170027e072>\u001b[0m in \u001b[0;36mextract_unique_referents\u001b[0;34m(doc1, doc2)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdoc_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruction_liste_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7aa87ea8581b>\u001b[0m in \u001b[0;36mconstruction_liste_similarity\u001b[0;34m(doc1, doc2)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_vector\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-773ce2c7bfd6>\u001b[0m in \u001b[0;36mcalculate_similarity\u001b[0;34m(token1, token2)\u001b[0m\n\u001b[1;32m      4\u001b[0m     <- similarity score between these two tokens\"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/spacy/tokens/token.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.similarity\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "extraction = extract_unique_referents(doc1, doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de44e5-9b03-48f1-9747-4ba8fad2f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    for dic in extraction[1]:\n",
    "        print(\"dic :\", dic, \" -> \", extraction[1][dic])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56004ba-1fb3-4255-92e9-df8b0c59d9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3aa8a2-98f3-4021-9b7d-66feb25e4b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1c28b-9954-4aab-8cb1-11ff81a7198d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
